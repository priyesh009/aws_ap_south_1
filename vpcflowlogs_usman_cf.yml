Parameters:
  ParentVPCStack:
    Description: 'VPC ID Stack name of parent VPC stack based on vpc/vpc-*azs.yaml template. '
    Type: String
  ExternalLogBucket:
    Description: 'Optional The name of an S3 bucket where you want to store flow logs. If you leave this empty, the Amazon S3 bucket is created for you.'
    Type: String
    Default: ''
  LogFilePrefix:
    Description: 'Optional The log file prefix.'
    Type: String
    Default: ''
  TrafficType:
    Description: 'The type of traffic to log.'
    Type: String
    Default: REJECT
    AllowedValues:
    - ACCEPT
    - REJECT
    - ALL
Conditions:
  InternalBucket: !Equals [!Ref ExternalLogBucket, '']
  ExternalBucket: !Not [!Equals [!Ref ExternalLogBucket, '']]
  HasLogFilePrefix: !Not [!Equals [!Ref LogFilePrefix, '']]
Resources:
  LogBucket:
    Condition: InternalBucket
    Type: 'AWS::S3::Bucket'
    Properties: {}
  LogBucketPolicy:
    Condition: InternalBucket
    Type: 'AWS::S3::BucketPolicy'
    Properties:
      Bucket: !Ref LogBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement: # https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-s3.html#flow-logs-s3-permissions
        - Sid: AWSLogDeliveryWrite
          Effect: Allow
          Principal:
            Service: 'delivery.logs.amazonaws.com'
          Action: 's3:PutObject'
          Resource: !If [HasLogFilePrefix, !Sub '${LogBucket.Arn}/${LogFilePrefix}/AWSLogs/${AWS::AccountId}/*', !Sub '${LogBucket.Arn}/AWSLogs/${AWS::AccountId}/*']
          Condition:
            StringEquals:
              's3:x-amz-acl': 'bucket-owner-full-control'
        - Sid: AWSLogDeliveryAclCheck
          Effect: Allow
          Principal:
            Service: 'delivery.logs.amazonaws.com'
          Action: 's3:GetBucketAcl'
          Resource: !GetAtt 'LogBucket.Arn'
        - Sid: AllowSSLRequestsOnly
          Effect: Deny
          Principal: '*'
          Action: 's3:*'
          Resource:
          - !GetAtt 'LogBucket.Arn'
          - !Sub '${LogBucket.Arn}/*'
          Condition:
            Bool:
              'aws:SecureTransport': false
  FlowLogInternalBucket:
    Condition: InternalBucket
    DependsOn: LogBucketPolicy
    Type: 'AWS::EC2::FlowLog'
    Properties:
      LogDestination: !If [HasLogFilePrefix, !Sub '${LogBucket.Arn}/${LogFilePrefix}/', !GetAtt 'LogBucket.Arn']
      LogDestinationType: s3
      ResourceId: vpc-b28cbeda
      ResourceType: 'VPC'
      TrafficType: !Ref TrafficType
  FlowLogExternalBucket:
    Condition: ExternalBucket
    Type: 'AWS::EC2::FlowLog'
    Properties:
      LogDestination: !If [HasLogFilePrefix, !Sub 'arn:aws:s3:::${ExternalLogBucket}/${LogFilePrefix}/', !Sub 'arn:aws:s3:::${ExternalLogBucket}']
      LogDestinationType: s3
      ResourceId: vpc-b28cbeda
      ResourceType: 'VPC'
      TrafficType: !Ref TrafficType
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties: 
      Code: 
        ZipFile: |
          import json
          import urllib
          import boto3
          import csv
          from netaddr import IPNetwork, IPAddress
          from datetime import datetime


          s3 = boto3.client('s3')


          def read_csv(filename):
              with open(filename) as csv_file:
                  csv_reader = csv.reader(csv_file, delimiter=',')
                  ips = []
                  for row in csv_reader:
                      ips.append(row[0])
              ips = ips[1:]
              return ips


          def lambda_handler(event, context):
              
              # get bucket name and key from event object
              bucket = event['Records'][0]['s3']['bucket']['name']
              key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
              
              try:
                  # read csv file contains CIDRs
                  ips = read_csv('smac-ips.csv')
                  
                  # get log files
                  res = s3.get_object(Bucket=bucket, Key=key)['Body'].read().decode()
                  
                  # preprocess log files
                  lines = res.split('\n')
                  lines = [i for i in lines if len(i.split(' ')) >= 3]
                  
                  # filtering logs
                  filter_logs = list(filter(lambda x: all([x.split(' ')[3] not in IPNetwork(i) for i in ips if len(x.split(' ')[3].split('.'))==4]), lines))
                  filter_logs = '\n'.join(filter_logs)
                  filter_logs = filter_logs.encode('utf-8')
                  
                  # Sending logs to Output bucket
                  response = s3.put_object(Bucket='smac-output-flow-logs', Body=filter_logs, Key=f'smac_flowlog_{str(datetime.now())}.log')
                  print(response)
                  return {'msg': 'Success'}
              except Exception as ex:
                  print(ex)
                  return {'msg': 'Failed'}
      Description: test lambda
      FunctionName: vpc_usman
      Handler: test_handler
      Role: arn:aws:iam::337769067848:role/LambdaFullAccess
      Runtime: python3.8 

Outputs:
  TemplateID:
    Description: 'cloudonaut.io template id.'
    Value: 'vpc/vpc-flow-logs-s3'
  TemplateVersion:
    Description: 'cloudonaut.io template version.'
    Value: '__VERSION__'
  StackName:
    Description: 'Stack name.'
    Value: !Sub '${AWS::StackName}'
  LogBucketName:
    Description: 'Log bucket name.'
    Value: !If [InternalBucket, !Ref LogBucket, !Ref ExternalLogBucket]